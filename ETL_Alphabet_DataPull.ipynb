{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this later from admin: !python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Imports](#first-bullet)\n",
    "1. [Functions](#1.0-bullet)\n",
    "2. [Main Pull](#2.0-bullet)\n",
    "3. [Cleaning](#3.0-bullet)  \n",
    " 3.1 [Pt 1 - Basic Cleaning](#3.1-bullet)  \n",
    " 3.2 [Pt 2 - Advanced Cleaning and feature engineering](#3.2-bullet)\n",
    "4. [Reference and Planning](#4.0-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not Run All cells - lots of intermittent cleaning included for reference\n",
    "\n",
    "Jump to Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Math\n",
    "import scipy as sp\n",
    "from random import randint\n",
    "from math import exp\n",
    "import operator\n",
    "\n",
    "#Scraping\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Youtube Data API Packages: \n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "from googleapiclient.discovery import build\n",
    "import argparse\n",
    "\"\"\"\n",
    "from googleapiclient.errors import HttpError\"\"\"\n",
    "\n",
    "#Modeling\n",
    "#from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "#NLP & Text Management\n",
    "'''from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer'''\n",
    "import re\n",
    "\n",
    "#Time analysis\n",
    "import time\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_file = open(\"./mykey.txt\", 'r')\n",
    "my_api_str = my_api_file.read()\n",
    "my_api_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = my_api_str\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_search_cat(q, max_results=50, sort_method='viewCount', token=None):\n",
    "    # Call the search.list method to retrieve results matching the specified query term.\n",
    "    #videoCategoryId is cat 10 - music\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "    developerKey=DEVELOPER_KEY)\n",
    "    \n",
    "    search_response = youtube.search().list(\n",
    "        q=q,\n",
    "        part='id,snippet',\n",
    "        pageToken=token,\n",
    "        maxResults=max_results,\n",
    "        order=sort_method,\n",
    "        videoCategoryId='10',\n",
    "        type='video'\n",
    "    ).execute()\n",
    "    \n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes output json list from search list and returns metadata on individual videos\n",
    "def youtube_search_video_cat(q='spinners', max_results=50, sort_method='viewCount', token=None):\n",
    "    \n",
    "    order = \"viewCount\"\n",
    "    q=q\n",
    "    max_results = max_results\n",
    "    sort_method = sort_method\n",
    "    token = token\n",
    "    location = None\n",
    "    location_radius = None\n",
    "    \n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
    "    developerKey=DEVELOPER_KEY)\n",
    "    \n",
    "    #Return list of matching records up to max_search\n",
    "    search_result = youtube_search_cat(q, max_results, sort_method, token)\n",
    "    next_tok = search_result['nextPageToken']\n",
    "    \n",
    "    videos_list = []\n",
    "    for search_result in search_result.get(\"items\", []):\n",
    "        \n",
    "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
    "            temp_dict_ = {}\n",
    "            #Available from initial search\n",
    "            temp_dict_['title'] = search_result['snippet']['title']  \n",
    "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
    "            \n",
    "            #Secondary call to find statistics results for individual video\n",
    "            response = youtube.videos().list(\n",
    "                part='statistics, snippet, contentDetails', \n",
    "                id=search_result['id']['videoId']\n",
    "                    ).execute()\n",
    "            #Relevant dictionaries\n",
    "            response_statistics = response['items'][0]['statistics']\n",
    "            response_snippet = response['items'][0]['snippet']\n",
    "            response_content= response['items'][0]['contentDetails']\n",
    "            \n",
    "            \n",
    "            snippet_list = ['publishedAt','channelId', 'description', \n",
    "                            'channelTitle', 'tags', 'categoryId', \n",
    "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
    "            for val in snippet_list:\n",
    "                try:\n",
    "                    temp_dict_[val] = response_snippet[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
    "            \n",
    "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
    "                          'dislikeCount', 'commentCount']\n",
    "            for val in stats_list:\n",
    "                try:\n",
    "                    temp_dict_[val] = response_statistics[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
    "            \n",
    "            for val in response_content.keys():\n",
    "                try:\n",
    "                    temp_dict_[val] = response_content[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
    "                    \n",
    "            #add back to main list\n",
    "            videos_list.append(temp_dict_)\n",
    "            \n",
    "    return videos_list, next_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automation\n",
    "def youtube_search_video_looper_cat(q,max_results=50, \n",
    "                                sort_method='viewCount', \n",
    "                                token=None, \n",
    "                                dl_path='./data/test/', dl_title='iteration'):\n",
    "    #find relevant stats, create dataframe of results, create running csvs\n",
    "    q=q,\n",
    "    max_results=max_results\n",
    "    sort_method = sort_method\n",
    "    token = token\n",
    "    \n",
    "    #create unique text file name\n",
    "    now_month = datetime.datetime.now().month\n",
    "    now_day = datetime.datetime.now().day\n",
    "    now_hour = datetime.datetime.now().hour\n",
    "    now_minute = datetime.datetime.now().minute\n",
    "    output_title = '{}{}_{}.{}_{}{}.csv'.format(dl_path, dl_title,  \n",
    "                                                  now_month, now_day, now_hour, now_minute)\n",
    "    \n",
    "    \n",
    "    #Cool Start to get next token\n",
    "    videos_list, next_tok = youtube_search_video_cat(q=q,\n",
    "                                                 max_results=max_results, \n",
    "                                                 sort_method=sort_method, \n",
    "                                                 token=token)\n",
    "    df_videos = pd.DataFrame(videos_list)\n",
    "    df_videos['request_token'] = next_tok #for assessment of run.\n",
    "\n",
    "    #Set up exit criteria - 2 counts in a row means its done\n",
    "    previous_results = 0\n",
    "    current_results = df_videos.shape[0]\n",
    "    \n",
    "    count=1\n",
    "    while previous_results != current_results:\n",
    "        try:\n",
    "\n",
    "            videos_list, next_tok = youtube_search_video_cat(q,\n",
    "                                                         max_results=max_results, \n",
    "                                                         sort_method=sort_method, \n",
    "                                                         token=next_tok)\n",
    "            print('    found page ', count)\n",
    "            df_small_vids = pd.DataFrame(videos_list)\n",
    "            df_small_vids['request_token'] = next_tok\n",
    "            df_videos = pd.concat([df_videos, df_small_vids], sort=False)\n",
    "\n",
    "\n",
    "            #save as we go\n",
    "            df_videos.to_csv(output_title, index=False)\n",
    "\n",
    "            _temp = current_results\n",
    "            current_results = df_videos.shape[0]\n",
    "            previous_results = _temp\n",
    "            count += 1\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            break\n",
    "    total_pulls = count\n",
    "    return df_videos, next_tok, total_pulls\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"2.0-bullet\"> 2. Data Pull</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "   foundall:  a\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  b\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  c\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  d\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  e\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "    found page  10\n",
      "   foundall:  f\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  g\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "    found page  10\n",
      "    found page  11\n",
      "   foundall:  h\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "   foundall:  i\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "   foundall:  j\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  k\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  l\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "   foundall:  m\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  n\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  o\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  p\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "    found page  10\n",
      "    found page  11\n",
      "   foundall:  q\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  r\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  x\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "   foundall:  t\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  u\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  v\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "    found page  9\n",
      "   foundall:  w\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  x\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  y\n",
      "    found page  1\n",
      "    found page  2\n",
      "    found page  3\n",
      "    found page  4\n",
      "    found page  5\n",
      "    found page  6\n",
      "    found page  7\n",
      "    found page  8\n",
      "   foundall:  z\n"
     ]
    }
   ],
   "source": [
    "#This is the major looping item used to collect data for alphabet_pull_init and cat10_relevance\n",
    "letters = 'abcdefghijklmnopqrxtuvwxyz'\n",
    "\n",
    "df_big_videos = pd.DataFrame()\n",
    "\n",
    "count=0\n",
    "for letter in letters:\n",
    "    df_videos, next_tok, total_pulls = youtube_search_video_looper_cat(\n",
    "        q=letter, sort_method='relevance', dl_path='./data/cat10_relevance/', dl_title='search'+letter+'2')\n",
    "    df_videos['letter_search'] = letter\n",
    "    print('   foundall: ', letter)\n",
    "    df_big_videos = pd.concat([df_big_videos, df_videos], sort=False)\n",
    "    df_big_videos.to_csv('./data/cat10_relevance/bigletters2.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10259, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big_videos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"3.0-bullet\"> 3. Cleanup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"3.1-bullet\">3.1 Nulls and Stuff</a>\n",
    "\n",
    "Note: This section was completed immediately after the main data pull was completed, where df_big_videos was still in memory.  \n",
    "Progress was saved and continued in 3.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_videos = pd.read_csv('./data/cat10_relevance/bigletters2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_big_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate indices from each individual request (came in batches of about 320)\n",
    "df_new = df_new.reset_index()\n",
    "df_new = df_new.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'caption', 'categoryId', 'channelId', 'channelTitle',\n",
       "       'commentCount', 'defaultLanguage', 'definition', 'description',\n",
       "       'dimension', 'dislikeCount', 'duration', 'favoriteCount',\n",
       "       'licensedContent', 'likeCount', 'liveBroadcastContent', 'projection',\n",
       "       'publishedAt', 'regionRestriction', 'tags', 'title', 'vidId',\n",
       "       'viewCount', 'request_token', 'letter_search', 'contentRating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates based on vidID - tags came in as a list, which the function can not otherwise handle.\n",
    "df_new = df_new.drop_duplicates(subset='vidId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171454286"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert nulls to 0, reform column as integers\n",
    "df_new['commentCount'] = df_new['commentCount'].map(lambda x: 0 if x == 'xxNoneFoundxx' else x )\n",
    "df_new['commentCount'] =df_new['commentCount'].astype('str').astype('int')\n",
    "df_new['commentCount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General the above process\n",
    "def clean_df_new_col(col):\n",
    "    df_new[col] = df_new[col].map(lambda x: 0 if x == 'xxNoneFoundxx' else x )\n",
    "    df_new[col] =df_new[col].astype('str').astype('int64')\n",
    "\n",
    "clean_df_new_col('favoriteCount')\n",
    "clean_df_new_col('likeCount')\n",
    "clean_df_new_col('viewCount')\n",
    "clean_df_new_col('dislikeCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tags lists to comma separated strings\n",
    "df_new['tags'] = df_new.tags.apply(lambda x: ', '.join(x) if x != 'xxNoneFoundxx' else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save point 1\n",
    "#df_new.to_csv('./data/alphabet_pull_init_01.10.18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"3.2-bullet\">3.2 Interpretation and Advanced Cleaning</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Converstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8670, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_data = pd.read_csv('./data/alphabet_pull_init_01.10.18.csv')\n",
    "df_data = df_new\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3= 3\n",
      "    21= 21\n",
      "    12= 12\n"
     ]
    }
   ],
   "source": [
    "df_temp = pd.DataFrame(df_data.loc[:, 'duration'])\n",
    "#Regex testing\n",
    "#Hours\n",
    "test_string1 = 'PT3H5M1S'\n",
    "print('    3=', int((re.findall('PT(.+)[01234567890]?H', test_string1))[0]))\n",
    "\n",
    "#Minutes\n",
    "test_string2 = 'PT32H21M1S'\n",
    "print('    21=', int(re.findall('[A-Z]([01234567890]*?M)', test_string2)[0].replace('M', '')))\n",
    "\n",
    "#Seconds\n",
    "test_string2 = 'PT23H12S'\n",
    "print('    12=', int(re.findall('[A-Z]([01234567890]*?S)', test_string2)[0].replace('S', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>hours</th>\n",
       "      <th>minutes</th>\n",
       "      <th>seconds</th>\n",
       "      <th>seconds_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT5M4S</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT6M3S</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT4M39S</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT3M54S</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT3M29S</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  duration  hours  minutes  seconds  seconds_tot\n",
       "0   PT5M4S      0        5        4          304\n",
       "1   PT6M3S      0        6        3          363\n",
       "2  PT4M39S      0        4       39          279\n",
       "3  PT3M54S      0        3       54          234\n",
       "4  PT3M29S      0        3       29          209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find digits in string and create column to hold\n",
    "df_temp['hours'] = df_temp['duration'].apply(lambda x: \n",
    "                                             0 if 'H' not in x else int((re.findall('PT(.+)[01234567890]?H', \n",
    "                                                                                    x))[0]))\n",
    "df_temp['minutes'] = df_temp['duration'].apply(lambda x: \n",
    "                                             0 if 'M' not in x else int(re.findall('[A-Z]([01234567890]*?M)',\n",
    "                                                                                   x)[0].replace('M', '')))\n",
    "df_temp['seconds'] = df_temp['duration'].apply(lambda x: \n",
    "                                             0 if 'S' not in x else int(re.findall('[A-Z]([01234567890]*?S)',\n",
    "                                                                                   x)[0].replace('S', '')))\n",
    "df_temp['seconds_tot'] = df_temp['seconds'] + df_temp['minutes'] * 60 + df_temp['hours'] * 60 * 60\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['duration'] = df_temp['seconds_tot'] #seems agressive to look too deeply into this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[:, ['publishedAt']] = pd.to_datetime(df_data.publishedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = df_data.publishedAt.dt.year - 2000\n",
    "df_data['month'] = df_data.publishedAt.dt.month\n",
    "df_data['month_day'] = df_data.publishedAt.dt.day\n",
    "df_data['year_day'] = df_data.publishedAt.dt.dayofyear\n",
    "df_data['week_day'] = df_data.publishedAt.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies \n",
    " * definition, (0 for sd, 1 for hd)\n",
    " * licensedContent (map)\n",
    " * Caption (map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[:, ['definition']] = df_data.definition.map({'sd':0, 'hd':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[:, ['licensedContent']] = df_data.licensedContent.map({False:0, True:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[:, ['caption']] = df_data.caption.map({False:0, True:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop, convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['categoryId', 'channelId', 'dimension', 'liveBroadcastContent', 'projection']\n",
    "#Changing this since I am destroying detail for the last columns, and for the drops\n",
    "df_clean = df_data.loc[:, [col for col in df_data.columns if col not in cols_to_drop]]\n",
    "df_clean['regionRestriction'] = df_clean['regionRestriction'].fillna(0).map(lambda x: 1 if x!= 0 else 0)\n",
    "df_clean['contentRating'] = df_clean['contentRating'].fillna(0).map(lambda x: 1 if x!= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['vidId','request_token', 'letter_search', ]\n",
    "cols_to_hold = ['defaultLanguage', 'regionRestriction','contentRating' ] #These are pretty sparse\n",
    "cols_for_analysis = [  'duration', 'licensedContent', 'definition',   #other \n",
    "                     'caption', 'channelTitle','tags', 'title', 'description', #text \n",
    "                     'publishedAt', 'year', 'month','month_day', 'year_day', 'week_day',  #dates\n",
    "                     'viewCount','likeCount','dislikeCount', 'favoriteCount','commentCount', #Results\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols_to_hold) + len(cols_for_analysis) + len(meta_cols) == len(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df_clean.columns if col not in meta_cols and col not in cols_to_hold and col not in cols_for_analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.to_csv('./data/alphabet_pull_round2_02.10.18.csv', index=False)\n",
    "df_clean.to_csv('./data/clean_data_nocomments_noviews_10.10.18.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
