{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this later from admin: !python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Math\n",
    "import scipy as sp\n",
    "from random import randint\n",
    "from math import exp\n",
    "import operator\n",
    "\n",
    "#Scraping\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#API Packages: \n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "\n",
    "#Modeling\n",
    "#from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "#NLP Processing\n",
    "'''from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer'''\n",
    "\n",
    "#Time analysis\n",
    "import time\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_file = open(\"./mykey.txt\", 'r')\n",
    "my_api_str = my_api_file.read()\n",
    "my_api_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = my_api_str\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all views over 400M https://www.youtube.com/playlist?list=PLirAqAtl_h2r5g8xGajEwdXd3x1sZh8hC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/results?filters=video&lclk=video&search_query=a%7Cb%7Cc%7Cd%7Ce%7Cf%7Cg%7Ch%7Ci%7Cj%7Ck%7Cl%7Cm%7Cn%7Co%7Cp%7Cq%7Cr%7Cs%7Ct%7Cu%7Cv%7Cw%7Cx%7Cy%7Cz%7C1%7C1%7C2%7C3%7C4%7C5%7C6%7C7%7C8%7C9%7C0&search_sort=video_view_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EYYYYYYYY\n",
    "https://github.com/youtube/api-samples/blob/master/python/search.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import argparse\\n\\nfrom googleapiclient.discovery import build\\nfrom googleapiclient.errors import HttpError'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This sample executes a search request for the specified search term.\n",
    "\n",
    "\"\"\"import argparse\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a list of videos according to search query 'q' as if you were typing it into the bar\n",
    "#The out put is a json list, predominantly used to get videoId's\n",
    "def youtube_search_list(q, max_results=50, sort_method='viewCount', token=None):\n",
    "  # Call the search.list method to retrieve results matching the specified query term.\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "    developerKey=DEVELOPER_KEY)\n",
    "\n",
    "  # Call the search.list method to retrieve results matching the specified query term.\n",
    "    search_response = youtube.search().list(\n",
    "        q=q,\n",
    "        part='id,snippet',\n",
    "        pageToken=token,\n",
    "        maxResults=max_results,\n",
    "        order=sort_method\n",
    "      ).execute()\n",
    "    \n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes output json list from search list and returns metadata on individual videos\n",
    "def youtube_search_video(q='spinners', max_results=50, sort_method='viewCount', token=None):\n",
    "    \n",
    "    order = \"viewCount\"\n",
    "    q=q\n",
    "    max_results = max_results\n",
    "    sort_method = sort_method\n",
    "    token = token\n",
    "    location = None\n",
    "    location_radius = None\n",
    "    \n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
    "    developerKey=DEVELOPER_KEY)\n",
    "    \n",
    "    #Return list of matching records up to max_search\n",
    "    search_result = youtube_search_list(q, max_results, sort_method, token)\n",
    "    next_tok = search_result['nextPageToken']\n",
    "    \n",
    "    videos_list = []\n",
    "    for search_result in search_result.get(\"items\", []):\n",
    "        \n",
    "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
    "            temp_dict_ = {}\n",
    "            #Available from initial search\n",
    "            temp_dict_['title'] = search_result['snippet']['title']  \n",
    "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
    "            \n",
    "            #Secondary call to find statistics results for individual video\n",
    "            response = youtube.videos().list(\n",
    "                part='statistics, snippet, contentDetails', \n",
    "                id=search_result['id']['videoId']\n",
    "                    ).execute()  \n",
    "            response_statistics = response['items'][0]['statistics']\n",
    "            response_snippet = response['items'][0]['snippet']\n",
    "            response_content= response['items'][0]['contentDetails']\n",
    "            \n",
    "            \n",
    "            snippet_list = ['publishedAt','channelId', 'description', \n",
    "                            'channelTitle', 'tags', 'categoryId', \n",
    "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
    "            for val in snippet_list:\n",
    "                try:\n",
    "                    temp_dict_[val] = response_snippet[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
    "            \n",
    "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
    "                          'dislikeCount', 'commentCount']\n",
    "            for val in stats_list:\n",
    "                try:\n",
    "                    temp_dict_[val] = response_statistics[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
    "            \n",
    "            for val in response_content.keys():\n",
    "                try:\n",
    "                    temp_dict_[val] = response_content[val]\n",
    "                except:\n",
    "                    #Not stored if not present\n",
    "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
    "                    \n",
    "            #add back to main list\n",
    "            videos_list.append(temp_dict_)\n",
    "            \n",
    "    return videos_list, next_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiate datframe using first results. \n",
    "#Allows for simple forloop\n",
    "videos_list, next_tok = youtube_search_video('Music Video')\n",
    "df_videos = pd.DataFrame(videos_list)\n",
    "df_videos['request_token'] = next_tok #for assessment of run.\n",
    "\n",
    "count=1\n",
    "while next_tok != 'last_page':\n",
    "\n",
    "    videos_list, next_tok = youtube_search_video(q='Music Video', token=next_tok)\n",
    "    print('    found page ', count)\n",
    "    df_small_vids = pd.DataFrame(videos_list)\n",
    "    df_small_vids['request_token'] = next_tok\n",
    "    df_videos = pd.concat([df_videos, df_small_vids], sort=False)\n",
    "    count += 1\n",
    "    \n",
    "    #create unique text file to track progress\n",
    "    now_month = datetime.datetime.now().month\n",
    "    now_day = datetime.datetime.now().day\n",
    "    now_hour = datetime.datetime.now().hour\n",
    "    now_minute = datetime.datetime.now().minute\n",
    "    output_title = './data/test/iteration{}_{}.{}_{}{}.csv'.format(count, now_month, now_day, now_hour, now_minute)\n",
    "    df_videos.to_csv(output_title, index=False)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos['viewCount'] = df_videos['viewCount'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351378"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos['viewCount'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it looks like youtube doesn't want to return too many videos like this\n",
    "#perhaps filtering by viewcount limits the effectiveness of relevance\n",
    "df_videos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     max irrelevant: 1300.0\n",
      "    final df size: 1625.0\n"
     ]
    }
   ],
   "source": [
    "#Given current shape, this is how many irrelevant videos I could use without \n",
    "#creating an inbalanced class\n",
    "print('     max irrelevant: {}\\n    final df size: {}'.format( df_videos.shape[0] / .25, df_videos.shape[0] / .25+df_videos.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num features: 24\n"
     ]
    }
   ],
   "source": [
    "print('    num features: {}'.format(df_videos.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Strategy -- Same as Readme\n",
    "I'd like to hit a dataframe row count of 10,000, with a minimum of 2k to support nlk processing. Additionally, I'm looking at expanding features by incorporating google trends results and topic analysis so this would require some more data points. To maintain a good feature / observation ratio.   \n",
    "\n",
    "Ultimately, I'm going to be aiming for regression-based analysis to project view count. \n",
    "\n",
    "There are two possible paths:  \n",
    "1. Stick with videos only, (see below for expansion)\n",
    "2. Expand scope to all videos and redefine the problem.  \n",
    "  \n",
    "  \n",
    "Ways to expand on data if youtube wants to stop here :\n",
    "* Most viewed vevo videos of all time (estimated to add 100)\n",
    "* Country, hip hop, pop, etc. hits (feels time intensive)\n",
    "* Do some additional searching from billboard or spotify to get specific songs, artists\n",
    "* Sourt by relevance instead (probably good for another 500)\n",
    "* Branch out from artists in top scoring (would help establish non-hit basis so I'm not overfitting Psy songs, for example. probably adds 2000 records easy)\n",
    "* Look at related search terms like remix, lyric video, etc, which would also expand the discussion to identify how people look for music.\n",
    "\n",
    "  \n",
    " There may be some duplicates in there, so I will have to consider how to treat those. I'm fine with including the same song twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hold off on this one until I figure out how to tie in google trends. \n",
    "Probably we want only comments from peak viewership plus marks at half peak on either side. \n",
    "Right now, most relevant /recent comments are all stupid meme stuff, e.g. who is watching in 2018'''\n",
    "\n",
    "#This function will grab 50 comments at a time from the video. \n",
    "def youtube_comment_list(video_id, max_results=50, sort_method='viewCount', token=None):\n",
    "  # Call the search.list method to retrieve results matching the specified query term.\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "    developerKey=DEVELOPER_KEY)\n",
    "\n",
    "  # Call the search.list method to retrieve results matching the specified query term.\n",
    "    search_response = youtube.commentThreads().list(\n",
    "        videoId = video_id,\n",
    "        part='snippet, replies',\n",
    "        pageToken=token,\n",
    "        order = 'relevance',\n",
    "        maxResults=max_results,\n",
    "      ).execute()\n",
    "    \n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sep 24th?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psy = youtube_comment_list('9bZkp7q19f0')\n",
    "\n",
    "psy['items'][1]['snippet']['topLevelComment']['snippet']['textDisplay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not super informative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
